# News Intelligence - Local LLM Integration

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ç–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Ollama, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–ª–∞–π–Ω –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö API.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å Docker

### 1. –ó–∞–ø—É—Å–∫ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM

```bash
# –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone <repository-url>
cd hack2025

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã —á–µ—Ä–µ–∑ Docker Compose
docker-compose up -d

# –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≥—Ä—É–∑–∫–∏ Ollama (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)
docker-compose logs -f ollama

# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Mistral
docker exec news-summarizer-ollama ollama pull mistral:7b-instruct
```

### 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `.env.local` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞:

```env
# –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
LOCAL_SUMMARY_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=mistral:7b-instruct
USE_LOCAL_ONLY=true

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: OpenAI API –∫–∞–∫ fallback
OPENAI_API_KEY=your-openai-key-here
```

### 3. –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pnpm install

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
pnpm dev
```

## üõ† –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# –°–∫–∞—á–∞–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫ —Å https://ollama.ai/download
```

### –ó–∞–ø—É—Å–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama
ollama serve

# –í –Ω–æ–≤–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
ollama pull mistral:7b-instruct

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
chmod +x scripts/setup-local-llm.sh
./scripts/setup-local-llm.sh mistral:7b-instruct
```

## üé® –î–∏–∑–∞–π–Ω –≤ —Å—Ç–∏–ª–µ Apple Intelligence

–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –±—ã–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω –≤ —Å—Ç–∏–ª–µ Apple Intelligence —Å:

- **–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞**: –ö–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ–∫—Ä—É—á–∏–≤–∞—é—Ç—Å—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ, –∫–∞–∫ –≤ –∫–∞—Ä—É—Å–µ–ª–∏
- **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏**: –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä (384px), —Å–∫—Ä—É–≥–ª–µ–Ω–Ω—ã–µ —É–≥–ª—ã, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ —Ç–µ–Ω–∏
- **–ü–ª–∞–≤–Ω—ã–µ –∞–Ω–∏–º–∞—Ü–∏–∏**: Hover-—ç—Ñ—Ñ–µ–∫—Ç—ã, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ—Ö–æ–¥—ã
- **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è**: –°—Ç—Ä–µ–ª–∫–∏ —Å backdrop-blur —ç—Ñ—Ñ–µ–∫—Ç–æ–º
- **–¢–µ–º–Ω–∞—è —Ç–µ–º–∞**: –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–µ–º–Ω–æ–π —Ç–µ–º—ã –≤ —Å—Ç–∏–ª–µ Apple

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞

–í –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–∞–Ω–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è:

- **–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤**: OpenAI API ‚Üî –õ–æ–∫–∞–ª—å–Ω–∞—è LLM
- **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏**: –í—ã–±–æ—Ä –º–µ–∂–¥—É Mistral, Llama 2, Code Llama, Phi
- **–î–ª–∏–Ω–∞ —Ä–µ–∑—é–º–µ**: –û—Ç 200 –¥–æ 600 —Å–∏–º–≤–æ–ª–æ–≤
- **–ö–∞—á–µ—Å—Ç–≤–æ**: –ù–∏–∑–∫–æ–µ (–±—ã—Å—Ç—Ä–æ) ‚Üî –í—ã—Å–æ–∫–æ–µ (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)

## üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|--------|----------|----------|--------------|
| `mistral:7b-instruct` | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è** |
| `llama2:7b-chat` | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | –•–æ—Ä–æ—à–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ |
| `phi:2.7b` | 2.7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | –ë—ã—Å—Ç—Ä–∞—è, –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è |
| `codellama:7b-instruct` | 7B | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ |

## üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –õ–æ–∫–∞–ª—å–Ω–∞—è LLM
- **–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏**: 2-5 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç–∞—Ç—å—é
- **–ü–∞–º—è—Ç—å**: 4-8 GB RAM (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏)
- **CPU**: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 4+ —è–¥–µ—Ä
- **GPU**: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ 2-3 —Ä–∞–∑–∞

### Fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
1. **–õ–æ–∫–∞–ª—å–Ω–∞—è LLM** (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞)
2. **OpenAI API** (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∫–ª—é—á)
3. **–≠–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è** (–≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç)

## üê≥ Docker –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # –î–ª—è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  web:
    build: .
    ports:
      - "3000:3000"
    environment:
      - LOCAL_SUMMARY_URL=http://ollama:11434/api/generate
      - OLLAMA_MODEL=mistral:7b-instruct
```

## üîç –û—Ç–ª–∞–¥–∫–∞

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
curl http://localhost:11434/api/tags

# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b-instruct",
    "prompt": "Summarize: The quick brown fox jumps over the lazy dog.",
    "stream": false
  }'
```

### –õ–æ–≥–∏ Docker

```bash
# –õ–æ–≥–∏ Ollama
docker-compose logs ollama

# –õ–æ–≥–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
docker-compose logs web

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–æ–≤
docker-compose restart
```

## üöÄ –î–µ–ø–ª–æ–π

### VPS —Å Docker

```bash
# –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
git clone <repository-url>
cd hack2025

# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
cp .env.example .env.local
nano .env.local

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å
docker-compose ps
```

### Vercel (—Ç–æ–ª—å–∫–æ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥)

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ Vercel Dashboard:
# LOCAL_SUMMARY_URL=https://your-ollama-server.com/api/generate
# OLLAMA_MODEL=mistral:7b-instruct
# USE_LOCAL_ONLY=true

vercel --prod
```

## üìù –ü—Ä–∏–º–µ—á–∞–Ω–∏—è

- **–ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏** –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 5-15 –º–∏–Ω—É—Ç
- **GPU —É—Å–∫–æ—Ä—è–µ—Ç** –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ 2-3 —Ä–∞–∑–∞
- **–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ** —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
- **Fallback** –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –ø—Ä–∏ —Å–±–æ—è—Ö LLM

## ü§ù –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `docker-compose logs`
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: `curl http://localhost:11434/api/tags`
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
4. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å—ã: `docker-compose restart`
