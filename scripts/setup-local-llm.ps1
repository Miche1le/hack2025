# PowerShell —Å–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: .\setup-local-llm.ps1 [model-name]

param(
    [string]$ModelName = "mistral:7b-instruct"
)

$OllamaHost = $env:OLLAMA_HOST
if (-not $OllamaHost) {
    $OllamaHost = "http://localhost:11434"
}

Write-Host "üöÄ Setting up local LLM with Ollama..." -ForegroundColor Green
Write-Host "Model: $ModelName" -ForegroundColor Cyan
Write-Host "Host: $OllamaHost" -ForegroundColor Cyan

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ Ollama
Write-Host "üì° Checking Ollama status..." -ForegroundColor Yellow
try {
    $response = Invoke-RestMethod -Uri "$OllamaHost/api/tags" -Method Get -ErrorAction Stop
    Write-Host "‚úÖ Ollama is running" -ForegroundColor Green
} catch {
    Write-Host "‚ùå Ollama is not running. Please start Ollama first:" -ForegroundColor Red
    Write-Host "   docker-compose up -d ollama" -ForegroundColor Yellow
    Write-Host "   or" -ForegroundColor Yellow
    Write-Host "   ollama serve" -ForegroundColor Yellow
    exit 1
}

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
Write-Host "üîç Checking if model $ModelName is available..." -ForegroundColor Yellow
try {
    $tagsResponse = Invoke-RestMethod -Uri "$OllamaHost/api/tags" -Method Get
    $modelExists = $tagsResponse.models | Where-Object { $_.name -eq $ModelName }
    
    if ($modelExists) {
        Write-Host "‚úÖ Model $ModelName is already available" -ForegroundColor Green
    } else {
        Write-Host "üì• Pulling model $ModelName..." -ForegroundColor Yellow
        & ollama pull $ModelName
        if ($LASTEXITCODE -eq 0) {
            Write-Host "‚úÖ Model $ModelName downloaded successfully" -ForegroundColor Green
        } else {
            Write-Host "‚ùå Failed to download model $ModelName" -ForegroundColor Red
            exit 1
        }
    }
} catch {
    Write-Host "‚ùå Error checking/downloading model: $($_.Exception.Message)" -ForegroundColor Red
    exit 1
}

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
Write-Host "üß™ Testing model..." -ForegroundColor Yellow
try {
    $testBody = @{
        model = $ModelName
        prompt = "Summarize this text in 2-3 sentences: The quick brown fox jumps over the lazy dog."
        stream = $false
    } | ConvertTo-Json

    $testResponse = Invoke-RestMethod -Uri "$OllamaHost/api/generate" -Method Post -Body $testBody -ContentType "application/json"
    
    if ($testResponse.response) {
        Write-Host "‚úÖ Model test successful" -ForegroundColor Green
        Write-Host "üìù Test response:" -ForegroundColor Cyan
        Write-Host $testResponse.response -ForegroundColor White
    } else {
        Write-Host "‚ùå Model test failed - no response received" -ForegroundColor Red
        exit 1
    }
} catch {
    Write-Host "‚ùå Model test failed: $($_.Exception.Message)" -ForegroundColor Red
    exit 1
}

Write-Host ""
Write-Host "üéâ Local LLM setup complete!" -ForegroundColor Green
Write-Host ""
Write-Host "üìã Next steps:" -ForegroundColor Cyan
Write-Host "1. Set environment variables:" -ForegroundColor Yellow
Write-Host "   `$env:LOCAL_SUMMARY_URL = `"$OllamaHost/api/generate`"" -ForegroundColor White
Write-Host "   `$env:OLLAMA_MODEL = `"$ModelName`"" -ForegroundColor White
Write-Host "   `$env:USE_LOCAL_ONLY = `"true`"" -ForegroundColor White
Write-Host ""
Write-Host "2. Or add to your .env.local file:" -ForegroundColor Yellow
Write-Host "   LOCAL_SUMMARY_URL=$OllamaHost/api/generate" -ForegroundColor White
Write-Host "   OLLAMA_MODEL=$ModelName" -ForegroundColor White
Write-Host "   USE_LOCAL_ONLY=true" -ForegroundColor White
Write-Host ""
Write-Host "3. Restart your application to use local LLM" -ForegroundColor Yellow
