Рисунок: пример горизонтально прокручиваемой секции карточек на сайте Apple — тёмная тема и панельные карточки.
Предлагаемый редизайн интерфейса и улучшения функционала агрегатора новостей
Визуальный стиль и UX-редизайн
[Рисунок: пример горизонтально прокручиваемой секции карточек на сайте Apple — тёмная тема и панельные карточки. Мы хотим воссоздать такой дизайн: плавная горизонтальная прокрутка и современная стилизация.]
Текущий интерфейс будет переработан под чистую, современную эстетику сайтов Apple и OpenAI. Ключевое изменение — внедрить горизонтально прокручиваемую ленту новостей и заново оформить каждую карточку с акцентом на визуал и читабельность:
• Горизонтальная лента карточек. Вместо вертикального списка новости показываются карточками в одном горизонтальном ряду (как карусель). Пользователь листает их вбок. Такой дизайн, вдохновлённый Apple, помогает быстро просматривать множество сюжетов без «бесконечной» прокрутки. На практике контейнер с карточками получает горизонтальную прокрутку (overflow-x), карточки — фиксированную ширину (flex-shrink-0), и включается scroll-snap, чтобы прокрутка «прилипала» к началу карточки. Это создаёт аккуратный эффект перелистывания.
• Дизайн и содержимое карточки. В каждой карточке — изображение (если доступно; иначе аккуратный плейсхолдер), крупный заголовок и краткая аннотация на 1–2 предложения. Также будет явный переход к первоисточнику — стрелка «→» или кнопка. Стили: скругления, лёгкая тень, плавные анимации появления и наведения. Текущая карточка уже имеет базовый белый фон и тень; мы доведём это до уровней Apple/OpenAI (больше «воздуха», крупнее заголовок, аккуратный CTA).
• Стилизация. Контрастные карточки на тёмном полотне (или наоборот — светлая тема). Используем утилитарные классы (например, Tailwind): rounded-lg, shadow-md/-lg, hover:-translate-y-0.5, transition. При первом появлении карточки — мягкое затухание/слайд-ин, чтобы интерфейс «дышал».
• Тёмная/светлая тема. Поддержим обе. Детектируем системную предпочтительную тему или дадим переключатель. В тёмной — глубокий «угольный» фон и светлый текст (как у ChatGPT); в светлой — нейтральный фон и тёмный текст. Везде держим достаточный контраст и аккуратные акценты.
[Рисунок: эстетика интерфейса ChatGPT — чистая вёрстка, крупная типографика заголовков и тёмная тема. Мы перенесём схожую минималистичную стилистику в новостную панель.]
• Типографика и компоновка. Крупные читаемые шрифты для заголовков, минималистичная графика. Заголовок страницы и подзаголовок (нынешние «News briefing dashboard» и т.п.) стилизуем в духе ChatGPT: смелый заголовок + лёгкий подзаголовок, много отступов. Форму и кнопки подчистим, сохранив простоту.
• Utility-first CSS. Опираться на Tailwind (уже в проекте): отступы, цвета, типографика, адаптивность, scroll-snap — всё без тяжёлого кастомного CSS. При необходимости — точечные правила.
Итог UX: пользователи быстро «пролистывают» понятные карточки-выжимки, как колоду карточек. Это прямо ложится на цель — «быстро понять, о чём новость, и стоит ли читать дальше».
Расширение функциональности UI
Помимо визуала, добавим интерактивные функции для персонализации:
• Фильтрация по ключевым словам в реальном времени. Сейчас фильтруем на сервере; добавим клиентскую фильтрацию «на лету» при наборе в поле Keywords. То есть уже полученную выборку будем мгновенно сужать на клиенте (по вхождению в заголовок/аннотацию), с неблокирующей задержкой (debounce). Это удобнее для «догранулировки» результатов без повторной загрузки.
• Сортировка/приоритизация. Добавим переключатель порядка:
  – По дате (Newest): уже есть на бэке; сделаем явный режим на фронте.
  – По источнику: сгруппировать/сортировать по домену/имени источника.
  – По релевантности: если ключевых слов несколько — ранжировать по количеству совпадений в title+summary (чем больше, тем выше).
  UI — dropdown или сегментный контрол «Sort: Latest / Source / Relevance». Сортировка выполняется на клиенте по данным уже в памяти.
• Интервал автообновления и статус. Поле интервала оставляем (оформим удобнее), валидируем (мин. 1 мин). Видимый индикатор «Последнее обновление: HH:MM». Кнопка «Обновить» — современный вид и лаконичная индикация загрузки (спиннер). Автообновление через setInterval остаётся; опционально добавим выключатель (pause), но в MVP достаточно текущей логики.
• Адаптивность. Горизонтальная лента на мобайле листается свайпом, на десктопе — колёсиком/трекпадом. Настроим ширину карточек по брейкпоинтам (например, на телефоне — почти во всю ширину с «подглядывающим» краем следующей карточки; на планшете — 2 на экран; на десктопе — 3 и больше). Заголовки и аннотации поджимаем line-clamp’ом, чтобы карточки не раздувались на малых экранах.
• Малые улучшения. Lazy-loading картинок с плейсхолдерами, «подсказка скролла» (виден край следующей карточки), управление стрелками (клавиатура/иконки) — опционально, если останется время.
Это закрывает «фильтры/приоритеты/частота обновлений» и поднимает UI на уровень современного дашборда.
ИИ-интеграция и очистка контента
• Проверка суммаризации через OpenAI. На бэке уже реализована абстрактивная сводка 2–3 предложения (через Chat Completions) с откатом к экстрактивной, если ключа нет или произошла ошибка. Подтвердим корректность обработки ошибок и кеширования, чтобы сводки не генерировались повторно в течение TTL.
• Удаление HTML-тегов из текста RSS. Многие ленты кладут HTML (br, ссылки и т.п.) в описание. Перед суммаризацией и выводом будем очищать контент: вырезать теги и декодировать сущности. Можно простой «грубый» вариант (регексп на <...>) или использовать sanitizer (например, DOMPurify в режимах Node/сервер), после чего оставлять только чистый текст. На фронте не использовать dangerouslySetInnerHTML для summaries — выводить как обычный текст. Это реализует идею из того коммита «может стоит убрать html теги?» и уберёт визуальный мусор/риски XSS.
• Локальная LLM как альтернатива. Чтобы не зависеть от внешнего API и иметь оффлайн-режим, можно подключить локальную МИ (например, Mistral 7B или Phi-2) через Ollama/LM Studio/HTTP-обёртку. План:
  – Поднять модель локально (Docker/сервис) и экспонировать REST.
  – В summarize() добавить ветку по переменной окружения (например, LOCAL_SUMMARY_URL): если задана — слать туда запрос с тем же промптом («Суммаризируй в 2–3 предложения…»).
  – Учесть производительность: 7B-модель может давать секунды на сводку; помогаем кешем, параллелизмом и ограничением числа элементов.
  – Деплой: при локальной LLM Vercel не подойдёт для самой модели; держать её на отдельном сервере/VPS, а фронт оставить на Vercel, или перенести всё в Docker на VPS.
  В результате даже без OPENAI_API_KEY сводки будут качественными (не только экстрактивными), а данные останутся локальными.
• Качество сводок. В промпте держим низкую температуру и ограничение объёма (2–3 предложения, фактичность). Для локальной модели может понадобиться слегка иной шаблон подсказки — протестируем и выберем формулировку, близкую к текущей.
Развёртывание
С нынешними изменениями фронт/бэкенд продолжают корректно работать на Vercel (Next.js 14, серверные API-роуты). CI/CD настроен — пуш/PR → проверки → деплой.
Если подключаем локальную LLM:
• Держать модель на отдельном хосте (VPS/Docker), а Next.js — на Vercel, общаться по защищённому REST.
• Или увести всё в Docker Compose на VPS: сервис Next.js + сервис LLM рядом (лучший контроль над ресурсами, подходит, если нужна NPU/GPU/AVX-оптимизация).
• Railway/Render подойдут для «тёплого» Node-сервиса, но тяжёлые модели там ограничены. Для стабильного продподхода — VPS/Hyperscaler VM.
Если остаёмся на OpenAI — ничего менять не нужно: ставим OPENAI_API_KEY в окружение Vercel и пользуемся.
Итог: получаем современный, удобный и технически устойчивый MVP-агрегатор: он выглядит профессионально, работает интерактивно, даёт понятные сводки и масштабируется под локальный ИИ при необходимости.
